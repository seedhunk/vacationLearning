# 一.机器学习基本概念

---

## 1. 什么是机器学习？

寻找一个函数，向函数输入样本，预测一个结果

###  分类：

| 类型                            | 输出形式 | 举例     |
| ------------------------------- | -------- | -------- |
| Regression（回归）              | 连续数值 | 气温预测 |
| Classification（分类）          | 离散标签 | 下棋     |
| Structured Learning（结构学习） | 实物     | 作画     |

---

## 2. 找函数的三个步骤（训练）

**第一步：写出一个带有未知参数的函数：**

![image-20250707213837465](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250707213837465.png)

其中：

- x：输入特征向量（feature）
- w：权重向量（weight）
- b：偏置项（bias）
- y：预测输出（model）

**第二步：损失函数（Loss Function）**：

![image-20250707214020449](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250707214020449.png)

损失值指的是预测值与实际值的误差，我们希望两者尽可能接近，即要求loss值尽可能小，最常用的是**均方误差（MSE）**

**第三步：最优化（Optimization）**：

采用梯度下降（Gradient Descent），整个过程可以理解为在找loss函数的最小值，通过计算w和b的微分来进行迭代

![](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250707214850145.png)

![image-20250707214516311](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250707214516311.png)

- n：学习率（learning rate，自己定义，即**Hyper parameter**）
- 小 n→ 学得慢；大 n → 可能震荡/不收敛

------

## 3.复杂函数的设定（对第一步骤的改写）

### 概念理解

线性函数过于简单？实际情况函数会很复杂，该如何设定？

- 分段线性函数可用常数项+线性函数表达：

![image-20250708151525904](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708151525904.png)

- 曲线函数可用分段线性函数逼近表示

![image-20250708151656623](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708151656623.png)



该如何表达基础的蓝色线性函数**（Hard Sigmoid）**？

![image-20250708151936489](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708151936489.png)



通过改变不同的w、b、c，可以制作不同的SIgmoid Function

![image-20250708152122790](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708152122790.png)



### 构造新的函数

![image-20250708152440246](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708152440246.png)



那么对于它的每一步，具体是怎么做的？以i，j：123为例子![image-20250708152916777](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708152916777.png)

![image-20250708153010358](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708153010358.png)

可以理解为矩阵的运算，结果为r1，r2，r3，然后r的矩阵再去通过sigmoid运算得到a的矩阵：

![image-20250708153224165](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708153224165.png)

![image-20250708153241637](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708153241637.png)

### 新函数如何用线性代数表示

通过上述过程，可以发现新函数的构造其实可以表示为：

![image-20250708153417066](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708153417066.png)

![image-20250708153655807](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708153655807.png)

其中w，b，c，b（一个b是常熟，一个b是矩阵）都是未知参数，这些未知参数可以组成一个θ矩阵

------



## 4.计算复杂函数Loss

![image-20250708154258428](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708154258428.png)



**小批量随机梯度下降：**

![image-20250708154529645](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250708154529645.png)

通过一个批次batch来更新（update）梯度（gradient），所有batch算过一遍后叫做一个回合（epoch）

Batch Size自己设定，即视作**Hype parameter**

------



## 5.Sigmoid -> ReLU

ReLU 修正线性单元：

![image-20250711092314300](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711092314300.png)

通过两个ReLU的叠加可以得到hard sigmoid

![image-20250711092701114](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711092701114.png)

当你不想使用sigmoid时，可以使用ReLU进行替换

![image-20250711092909431](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711092909431.png)

------



## 取个好听的名字吧

![image-20250711094135792](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711094135792.png)

Neuron 神经元（用烂了不用了）

------



# 二.Pytorch Tutorial

## 1.Load Data

### 读取：

torch.utils.data.Dataset：将资料一柄一柄地读入

torch.utils.data.DataLoader：将dataset的资料整理成Batch

![image-20250711095536524](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711095536524.png)

![image-20250711095731895](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711095731895.png)

![image-20250711095827991](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711095827991.png)

------

### Tensors

可以理解为矩阵，表现形式如下：

![image-20250711100308952](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100308952.png)

![image-20250711100326464](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100326464.png)

**如何产生？**

![image-20250711100412107](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100412107.png)

一些运算：

+、-、pose()、sum()、mean()

transpose()：

![image-20250711100819620](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100819620.png)

squeeze()、unsqueeze()：

![image-20250711100855909](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100855909.png)

![image-20250711100952913](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711100952913.png)

cat()：

![image-20250711101225924](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711101225924.png)

### 计算梯度

![image-20250711101934413](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711101934413.png)

------



## 2.Training ＆ Testing

### 定义layer

linear layer的设定：

![image-20250711102514991](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711102514991.png)

 

![image-20250711102728810](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711102728810.png)



**用pytorch怎么写？**

![image-20250711102912867](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711102912867.png)



**当使用Non-Linear Activatin Function时，如sigmoid或者ReLU**

![image-20250711103113079](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711103113079.png)

### 定义自己的神经网络

![image-20250711112405014](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711112405014.png)



### 定义Loss Functions

- Mean Squared Error（for regression tasks）

​				criterion = nn.MSEloss()

- Cross Entropy（for classification tasks）

​				criterion = nn.CrossEntropyLoss()

- loss = criterion（model_output, expected_value）\



### 最优化

使用最基本的梯度下降：

optimizer = torch.optim.SGD(model.parameters(), lr, momentum = 0)

![image-20250711114041766](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711114041766.png)

------



## 3.全流程速通！

```python
#read data via Mydataset
dataset = Mydataset(file)
#put dataset into Dataloader
tr_set = DataLoader(dataset, 16, shuffle=True) 
#construct model and move to device (cpu/cuda)
model = myModel().to(device)
#set loss function
criterion = nn.MSELoss()
#set optimizer
optimizer = torch.optim.SGD(model.parameters(), 0.1)


for epoch in range(n_epochs):
    model.train()
    for x, y in tr_set:
        optimizer.zero_grad()
        x, y = x.to(device), y.to(device)
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()

model.eval()
total_loss = 0
for x, y in dv_set:
     x, y = x.to(device), y.to(device)
        with torch.no_grad():
            pred = model(x)
            loss = criterion(pred, y)
        total_loss += loss.cpu().item() * len(x)
avg_loss = total_loss / len(dv_set.dataset)

model.eval()
preds = []
for x in tt_set:
    x = x.to(device)
    with torch.no_grad():
        pred = model(x)
        preds.append(pred.cpu())
```



两个过程的解释：

![image-20250711125400501](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711125400501.png)

------



## 4.存储和下载

![image-20250711125521714](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711125521714.png)

------



# 三.深度学习

## Neutral network

deep = many hidden layers

**layer之间如何连接决定了整个功能的好坏**

![image-20250711172611138](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711172611138.png)

**用Matrix Operation表示：**

![image-20250711172142953](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711172142953.png)



![image-20250711172120742](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711172120742.png)

------



## 一个简单的例子：

![image-20250711172459663](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711172459663.png)

------



## 神经网络的Loss

对于一整个neural network，它的Loss用交叉熵的方式计算

![image-20250711173401844](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711173401844.png)



**然后将所有训练集的loss总和后选择好的structure，再从好的Structure里面进行梯度下降得到最好的参数集：**

![image-20250711174040004](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711174040004.png)

![image-20250711174104930](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711174104930.png)

------



## 反向传播(Backpropagation)

**backpropagation在做什么事？**神经网络中的参数通常有上百万个，反向传播可以有效地提高梯度下降过程的计算效率

### Chain Rule(链式法则)

![image-20250711175132108](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711175132108.png)

------



### 整体思想

利用链式法则，选取单个神经元的过程，可以看到：

![image-20250711175438356](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711175438356.png)

计算forward pass的过程，其实结果就是对应的x：

![image-20250711175650398](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711175650398.png)

计算backword pass：

![image-20250711181158669](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711181158669.png)



**这是你可以发现整个过程是可以递归的，即可以先算出最后面，然后往前做计算**

![image-20250711182026317](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250711182026317.png)