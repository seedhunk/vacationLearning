# 一.机器学习任务攻略

当你对训练结果不满意时，我们应该做些什么呢？

##  检查Loss on training data

![image-20250715163206738](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715163206738.png)

### large—检查Model Bias？Optimization？

也许是设定的函数过于简单了，即使找到了函数中能让loss最小的参数集，整体的loss还是很大的，这并不是我们所希望的

此时我们应该增加函数的弹性，即增加更多的features并可以使用sigmoid或者ReLU去描述我们的函数（more neurons，layers）

![image-20250715163526393](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715163526393.png)

------

也许函数已经够复杂了，但gradient decent没有办法找到其中的最优解

![image-20250715163959682](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715163959682.png)

------

 如何知道到底是model bias的问题还是Optimization的问题呢？

**先去使用一个很简单的函数去测试，然后再用复杂的，如果复杂的loss结果还不如简单的，那说明是Optimization的问题**

![image-20250715170129263](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715170129263.png)

![image-20250715170152107](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715170152107.png)

------

### small—检查loss on testing data

当训练集loss小，但测试集结果loss大，我们把这种情况成为overfitting过拟合

为什么会出现overfitting过拟合？

![image-20250715180640842](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715180640842.png)

如何解决？

- 增加训练集（收集资料或使用data augmentation数据增强）
- 根据自己的理解，去给model一些限制

—限制方法

![image-20250715181205589](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715181205589.png)

 

------

**Mismatch错配**

当你的训练资料和测试资料的产生方式不一致，即不是一类的资料，会出现错配的情况

![image-20250715184155087](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715184155087.png)

------



## 交叉验证

如果直接将训练集训练的最好的model交给公开测试集，也许会得到一个很好的结果，但不能保证在私有训练集上也会有，因此我们可以将训练集分为几份，其中一份作为验证集充当测试集的测试前置

![image-20250715183640893](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250715183640893.png)

------



# 二.类神经网络训练不起来怎么办？

## 1.局部最小值(local minima)与鞍点(saddle point)

gradient为0的点不一定是local minima或maxima，也有可能是saddle point

这三种点都可以称为critical point

![image-20250716173915311](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716173915311.png)

------

**如何判断到底是那种呢？**

我们需要知道loss function长什么样子才能更好地判断，那么loss function该怎么表示呢？在一个特定点周围的function我们可以表示为：

![image-20250716175131124](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716175131124.png)

绿色的部分是一个向量补足，因此当为critical point时，绿色部分为0

红色部分是矩阵运算，当H的特征值（eigen values）全为正，即为local minima；全为负即为local maxima；有正有负即为鞍点

![image-20250716175626586](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716175626586.png)

**（实操一般不用）**当这个点是鞍点时，我们可以通过H来选择对应方向去继续降低loss，我们只需要找出H的负特征值对应的特征向量，朝这个向量走即可：

![image-20250716181007298](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716181007298.png)

------



## 2.批次(batch)和动量(momentum)

### 为什么要用batch来做训练？

下面看两个极端的例子，当batch=full时，即训练完所以资料才做loss计算并update参数，或当batch=1时，训练一份资料就做一次loss计算并update参数：

![image-20250716182336036](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716182336036.png)

**我们会发现左边蓄力时间长，但更准确，右边蓄力时间短，但更混乱**

------

**当考虑平行运算时，batch size大的计算时长并不一定就会特别长；但batch size越小就意味着跑完一次epoch花费的时间会更长：**

![image-20250716182906268](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716182906268.png)

![image-20250716183107058](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716183107058.png)

------

### batch size选小点效果会更好？

- 每次update会让函数有些不一样，更利于下一次找到更小的loss

![image-20250716184019105](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716184019105.png)

- large batch会更倾向于找到sharp minima，这会导致训练与测试的loss差距变大

![image-20250716184153579](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716184153579.png)

------

### Small Batch V.S. Large Batch

![image-20250716184335342](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250716184335342.png)

------

### 动量Momentum

动量法是一种**优化算法加速策略**，常用于训练神经网络时对梯度下降法进行改进。

####  核心思想：

> **Movement = 上一步的移动（Momentum） - 当前梯度的影响**

即：更新参数时，不仅考虑当前的梯度值，还会参考“之前移动的方向”，从而在整体上形成更稳定、更快的下降路径。

数学推导：

动量的核心变量为m，表示第i次迭代时的**移动量（Movement）**：
$$
m^0 = 0 m^1 = -\eta g^0 m^2 = \lambda m^1 - \eta g^1 ... m^i = \lambda m^{i-1} - \eta g^{i-1}
$$
其中：

- g^i：第i次迭代的梯度
- η：学习率
- λ：动量因子（通常取 0.9 左右），控制上一次移动的影响力
- θ^i：第i次的参数更新

更新公式为：
$$
\theta^{i+1} = \theta^i + m^{i+1}
$$
📌 **注意：** `Movement` 是前一步的移动乘以 λ，然后减去当前梯度乘以学习率。

![image-20250718172426809](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718172426809.png)

------

#### 如何理解整个过程？

- 🔴 **红色箭头**：当前的梯度（负方向）
- 🔵 **蓝色虚线箭头**：上一步的移动方向（Momentum）
- 🔵 **蓝色实线箭头**：实际的移动方向（综合考虑了前一步和当前梯度）

- 在陡峭斜坡上，动量会加快下降速度。
- 在平缓区域，动量可以**保持惯性**，避免陷入局部极小值。
- 在鞍点（梯度为 0）附近，仍然可能因上一步的动量继续前进，**避免停滞**。![image-20250718172651837](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718172651837.png)

------

#### 动量法的优点

- 减少震荡，使优化路径更平滑
- 加快收敛速度，特别是在长谷地形中
- 帮助跳出局部最优或鞍点

------



## 3.自动调整学习率(Learning rate)

### 训练停滞（Training Stuck）并不等于梯度变小（Small Gradient）

 很多人误以为训练停滞意味着模型进入了梯度很小、接近收敛的区域，但实际上，有可能是模型卡在了一个**“鞍点”或平坦的区域**。

左侧图:

- 图中的曲线是损失函数的形状，展示了一个非凸函数，有多个低谷和一个鞍点。
- 绿色箭头表示参数在平坦区域中左右摆动。
- 虽然梯度在鞍点附近变小，但并不是收敛，而是训练被卡住了。

右上图（Loss vs Iteration）:

- 显示损失（loss）随着训练迭代的变化。
- 可以看到在大约 100 次迭代后，loss 几乎稳定在较小值，表面上看似收敛。

右下图（Gradient Norm vs Iteration）：

- 展示的是梯度的范数（norm of gradient），即每一步梯度的大小。
- 虽然 loss 看起来趋于稳定，但**梯度依然存在剧烈波动**（图中红圈处）。
- 表明模型仍然在某些方向有“下坡”空间，只是**没有有效移动或找到下降方向**。

![image-20250718174831526](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718174831526.png)

这说明：

- **Loss 平稳 ≠ 模型已收敛**
- **梯度不小 ≠ 停止学习**
- 模型可能被卡在鞍点附近或平坦区域，尤其在高维空间中，这种情况更常见。
- **动量法（Momentum）或二阶优化器**可以在这种情况下帮助模型“冲出”鞍点。

- 训练监控不应仅关注 Loss 曲线，还应关注梯度信息。
- **Training stuck 的本质**：优化器没有有效地找到下降方向，而不是没有方向可走。
- 可以考虑：
  - 使用 Momentum / Adam 等优化器；
  - 学习率衰减策略；
  - 引入扰动逃离鞍点。

------



### 自动调整学习率解决震荡导致的训练停止

将学习率η替换为η/σ

![image-20250718180033304](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718180033304.png)

其计算方式为：

**（不使用这个版本）**

![image-20250718175954745](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718175954745.png)

**（推荐使用）**

![image-20250718180653654](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718180653654.png)

------



### Learning Rate Scheduling

前面这样虽然能解决震荡，但会出现暴走的情况，因为随着步伐的前进，gradient由于比较平滑导致每次的累计很小，分母部分过小就会导致结果变得超大然后暴走

![image-20250718182302750](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718182302750.png)

如何解决？

将η看作一个随着时间改变的量即可，可以使用decay和warm up两种

![image-20250718182239600](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718182239600.png)

------



### 最常用的Optimization的策略

Adam：RMSProp + Momentum

![image-20250718182636922](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718182636922.png)

------



## 4.Classification

类比regression的做法，classification该怎么做？

![image-20250718213709625](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718213709625.png)

soft-max可以理解为将每个结果转换为概率：

![image-20250718213936727](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718213936727.png)

计算分类loss时，pytorch中soft-max与交叉熵（就使用这个）绑定

![image-20250718214521507](C:\Users\bri\AppData\Roaming\Typora\typora-user-images\image-20250718214521507.png)